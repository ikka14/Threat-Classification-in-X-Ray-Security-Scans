{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd6159b",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "# Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c9e523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T13:30:25.853198Z",
     "iopub.status.busy": "2022-05-18T13:30:25.852680Z",
     "iopub.status.idle": "2022-05-18T13:30:28.411267Z",
     "shell.execute_reply": "2022-05-18T13:30:28.410384Z",
     "shell.execute_reply.started": "2022-05-18T13:30:25.853142Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.integrate\n",
    "import scipy.special\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPool2D,\n",
    "    MaxPooling2D,\n",
    "    Rescaling,\n",
    ")\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import applications, layers, regularizers\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146a223",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "# Class Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1d60b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T01:37:26.428360Z",
     "iopub.status.busy": "2022-05-19T01:37:26.427830Z",
     "iopub.status.idle": "2022-05-19T01:37:26.469324Z",
     "shell.execute_reply": "2022-05-19T01:37:26.468303Z",
     "shell.execute_reply.started": "2022-05-19T01:37:26.428293Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Constants_model:\n",
    "    '''This class is used to define constants that will be used throughout the project'''\n",
    "    train_path = \"_data/allray/train/\"\n",
    "    test_path = \"_data/allray/test/\"\n",
    "    target_names = ['Gun','Knife','Nothing','Pliers','Scissors','Wrench']\n",
    "\n",
    "\n",
    "class Xray_models(Constants_model):\n",
    "    '''Create a xray class model which can predict on xray images'''\n",
    "    \n",
    "    def __init__(self, optimizer, img_rows, img_cols, batch_size):\n",
    "        '''Initialize the parameters important for the model'''\n",
    "        self.optimizer = optimizer\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def create_data_gen(self):\n",
    "        '''\n",
    "        Create a image data generator where it is specified the split on the training set and val set\n",
    "        returns two image data generators one for training data and one for test data as a tuple\n",
    "        '''\n",
    "        train_datagen = ImageDataGenerator(validation_split=0.2)\n",
    "        test_datagen = ImageDataGenerator()\n",
    "        return (train_datagen, test_datagen)\n",
    "\n",
    "    def create_data_gen_aug(self):\n",
    "        '''\n",
    "        Create a data generator where it is specified the split and the data augmentation\n",
    "        returns two image data generators one for training data augmented and one for test data as a tuple\n",
    "        '''\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            validation_split=0.2,\n",
    "            rotation_range=45,  \n",
    "            horizontal_flip=True, \n",
    "            vertical_flip=True, \n",
    "        ) \n",
    "        test_datagen = ImageDataGenerator()\n",
    "        return (train_datagen, test_datagen)\n",
    "\n",
    "    def get_images_(self, augmented=False, hard=False):\n",
    "        '''\n",
    "        Create the train,validation and test generator.Each contains the images and their labels\n",
    "        with flow from directory the data is gathered and resized to the dim specified then suffled \n",
    "        returns three generator one for train,for validation and test in form of a tuple\n",
    "        '''\n",
    "        # load data either with augmentation or in original form\n",
    "        if augmented == True:\n",
    "            train_datagen, test_datagen = self.create_data_gen_aug()\n",
    "        else:\n",
    "            train_datagen, test_datagen = self.create_data_gen()\n",
    "\n",
    "        # create training set from allray directory   \n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            self.train_path,\n",
    "            target_size=(self.img_rows, self.img_cols),\n",
    "            batch_size=self.batch_size,\n",
    "            seed=42,\n",
    "            shuffle=True,\n",
    "            class_mode=\"categorical\",\n",
    "            subset=\"training\",\n",
    "        )\n",
    "        # create validation set from allray directory\n",
    "        val_generator = train_datagen.flow_from_directory(\n",
    "            self.train_path,\n",
    "            target_size=(self.img_rows, self.img_cols),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            seed=42,\n",
    "            class_mode=\"categorical\",\n",
    "            subset=\"validation\",\n",
    "        )\n",
    "        # create test set from allray directory\n",
    "        test_generator = test_datagen.flow_from_directory(\n",
    "            self.test_path,\n",
    "            target_size=(self.img_rows, self.img_cols),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            class_mode=\"categorical\",\n",
    "        )\n",
    "\n",
    "        return (train_generator, val_generator, test_generator)\n",
    "\n",
    "    def create_model(self, model_type):\n",
    "        '''\n",
    "        Create one of the models specified as a parameter,if model mentioned is not present return an error message\n",
    "        for each case we return the model specified   \n",
    "        '''\n",
    "        if model_type == \"vgg16\":\n",
    "            \n",
    "            cnn2 = tf.keras.models.Sequential()\n",
    "            cnn2.add(tf.keras.layers.Rescaling(scale=1 / 127.5, offset=-1))\n",
    "            cnn2.add(tf.keras.layers.Conv2D(filters=48, kernel_size=3, activation='relu', input_shape=[self.img_rows, self.img_cols, 3]))\n",
    "            cnn2.add(tf.keras.layers.Conv2D(filters=48, kernel_size=3, activation='relu'))\n",
    "            cnn2.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "            cnn2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "            cnn2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "            cnn2.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "            cnn2.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "            cnn2.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "            cnn2.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "            cnn2.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "            cnn2.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu'))\n",
    "            cnn2.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu'))\n",
    "            cnn2.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu'))\n",
    "            cnn2.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "            cnn2.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "            cnn2.add(tf.keras.layers.Dropout(0.5))\n",
    "            cnn2.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "            cnn2.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "            cnn2.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "            cnn2.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "            # set metrics of interest to recall\n",
    "            cnn2.compile(optimizer=self.optimizer, loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.Recall()])\n",
    "            return cnn2\n",
    "        \n",
    "        elif model_type == \"transfer_learning\":\n",
    "            \n",
    "            base_model = applications.EfficientNetV2S(\n",
    "                weights='imagenet',  # Load weights pre-trained on ImageNet\n",
    "                input_shape=(128, 128, 3),\n",
    "                include_top=False)\n",
    "            \n",
    "            base_model.trainable = False\n",
    "            inputs = keras.Input(shape=(self.img_rows, self.img_cols, 3))\n",
    "            # rescale pixels between -1 and 1, needed for transfer learning model\n",
    "            scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
    "            x = scale_layer(inputs)\n",
    "            x = base_model(inputs, training=False)\n",
    "            x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "            outputs = keras.layers.Dense(6, activation=\"softmax\")(x)\n",
    "            model = keras.Model(inputs, outputs)\n",
    "            model.compile(\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss=\"categorical_crossentropy\",\n",
    "                # set metrics of interest to recall\n",
    "                metrics=[tf.keras.metrics.Recall()],\n",
    "            )\n",
    "            return model\n",
    "        \n",
    "        elif model_type == \"own_model\":\n",
    "            \n",
    "            cnn = tf.keras.models.Sequential()\n",
    "            cnn.add(tf.keras.layers.Conv2D(filters=48, kernel_size=3, activation='relu', input_shape=[self.img_rows, self.img_cols,3]))\n",
    "            cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "            cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "            cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "            cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "            cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "            cnn.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "            cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "            cnn.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu'))\n",
    "            cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "            cnn.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "            cnn.add(tf.keras.layers.Dropout(0.5))\n",
    "            cnn.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "            cnn.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "            cnn.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "            # set metrics of interest to recall\n",
    "            cnn.compile(optimizer=self.optimizer, loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.Recall()])\n",
    "            return cnn\n",
    "        \n",
    "        elif model_type == \"svm\":\n",
    "            \n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(filters=32, padding=\"same\", activation=\"relu\", kernel_size=3, strides=2, input_shape=(self.img_rows, self.img_cols, 3)))\n",
    "            model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "            model.add(Conv2D(filters=32, padding=\"same\", activation=\"relu\", kernel_size=3))\n",
    "            model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(128, activation=\"relu\"))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(6, kernel_regularizer=regularizers.l2(0.01), activation=\"softmax\"))\n",
    "            model.compile(\n",
    "                optimizer=self.optimizer,\n",
    "                # use hinge for two class SVM\n",
    "                loss = 'squared_hinge',\n",
    "                # set metrics of interest to recall\n",
    "                metrics=[tf.keras.metrics.Recall()])\n",
    "            return model\n",
    "        else:\n",
    "            return 'Model not found.Please specify another model'\n",
    "\n",
    "    def fit_model(self, model_type, epochs, augmented=False, hard_test=False):\n",
    "        '''\n",
    "        This function will fit the model to the data.\n",
    "        It will check if the data is augmented or not and get the specific generator.\n",
    "        It will define the type of callbacks that are used \n",
    "        returns the model fitted on the training set and the test generator\n",
    "        '''\n",
    "        my_callbacks = [\n",
    "            # stop model if it does not increase after 20 epochs\n",
    "            tf.keras.callbacks.EarlyStopping(patience=20),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=\"model.{epoch:02d}-{val_loss:.2f}.h5\",\n",
    "                monitor=\"val_loss\",\n",
    "                mode=\"min\",\n",
    "                save_best_only=True,\n",
    "                verbose=1,\n",
    "            ), tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")]\n",
    "        # check for augmentation\n",
    "        if augmented == True:\n",
    "            train_generator, val_generator, test_generator = self.get_images_(augmented=True, hard=False)\n",
    "        else:\n",
    "            train_generator, val_generator, test_generator = self.get_images_(augmented=False, hard=False)\n",
    "        # get the specified model\n",
    "        model = self.create_model(model_type)\n",
    "        model.fit(\n",
    "            x=train_generator,\n",
    "            validation_data=val_generator,\n",
    "            epochs=epochs,\n",
    "            callbacks=my_callbacks,\n",
    "        )\n",
    "        return (model, test_generator)\n",
    "\n",
    "    def evaluate_model(self, model_type, epochs, augmented=False, hard_test=False):\n",
    "        '''\n",
    "        This function gets the fitted model and then generates the predictions on the test set\n",
    "        return the predicted classes, the true classes and the model\n",
    "        '''\n",
    "        # get fitted model and test data\n",
    "        model, test_generator = self.fit_model(model_type, epochs, augmented)\n",
    "        STEP_SIZE = test_generator.n // test_generator.batch_size\n",
    "        prediction_classes = np.array([])\n",
    "        true_classes = np.array([])\n",
    "        \n",
    "        i = 0\n",
    "        # contenate batches together to allow for creation of confusion matrix\n",
    "        for x, y in test_generator:\n",
    "            i = i + 1\n",
    "            if i > STEP_SIZE + 1:\n",
    "                break\n",
    "            prediction_classes = np.concatenate(\n",
    "                [prediction_classes, np.argmax(model.predict(x), axis=-1)]\n",
    "            )\n",
    "            true_classes = np.concatenate([true_classes, np.argmax(y, axis=-1)])\n",
    "        return (prediction_classes, true_classes, model)\n",
    "    \n",
    "    def class_report(self, model_type, epochs, augmented=False, hard_test=False):\n",
    "        '''\n",
    "        Function that generates the classification report after getting the prediction classes and true classes\n",
    "        prints a classification report specific for the mentioned model\n",
    "        '''\n",
    "        prediction_classes, true_classes = self.evaluate_model(\n",
    "            model_type, epochs, augmented)\n",
    "        print(classification_report(true_classes, prediction_classes, target_names=self.target_names))\n",
    "\n",
    "    def confusion_matrix(self, model_type, epochs, augmented=False, hard_test=False):\n",
    "        '''\n",
    "        Function that generates the confusion matrix after getting the prediction classes and true classes\n",
    "        prints a confusion matrix specific for the mentioned model\n",
    "        '''\n",
    "        prediction_classes, true_classes = self.evaluate_model(model_type, epochs, augmented)\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            true_classes,\n",
    "            prediction_classes,\n",
    "            display_labels=traget_names,\n",
    "            ax=ax)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6931ef",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47510e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T01:37:27.145438Z",
     "iopub.status.busy": "2022-05-19T01:37:27.144943Z",
     "iopub.status.idle": "2022-05-19T01:37:27.152952Z",
     "shell.execute_reply": "2022-05-19T01:37:27.151868Z",
     "shell.execute_reply.started": "2022-05-19T01:37:27.145382Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use Adam optimizer\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0002)\n",
    "\n",
    "# set images size to 128*128 and batch size to 64\n",
    "xray = Xray_models(opt, 128, 128, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b74353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T02:01:16.778322Z",
     "iopub.status.busy": "2022-05-19T02:01:16.777814Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create and train different models\n",
    "model1 = xray.evaluate_model(\"svm\", epochs=40, augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = xray.evaluate_model(\"own_model\", epochs=30, augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = xray.evaluate_model(\"vgg16\", epochs=40, augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = xray.evaluate_model(\"transfer_learning\", epochs=40, augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"SVM\":model1,\"Own_Model\":model2,\"VGG16\":model3,\"CNN-TL\":model4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d39845",
   "metadata": {},
   "source": [
    "# Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc14705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print confusion matrix for all models\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25, 15))\n",
    "for model_name, ax in zip(models.items(), axes.flatten()):\n",
    "    name, model = model_name\n",
    "    prediction_classes, true_classes,model_x = model\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        true_classes,\n",
    "        prediction_classes,\n",
    "        display_labels=['Gun','Knife','Nothing','Pliers','Scissors','Wrench'],\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.title.set_text(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b818cb-9142-4c6f-9cc5-fba4441ffcfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T13:48:45.731996Z",
     "iopub.status.busy": "2022-05-19T13:48:45.731227Z",
     "iopub.status.idle": "2022-05-19T13:48:45.797195Z",
     "shell.execute_reply": "2022-05-19T13:48:45.796529Z",
     "shell.execute_reply.started": "2022-05-19T13:48:45.731934Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print classification report for all models\n",
    "target_names = ['Gun','Knife','Nothing','Pliers','Scissors','Wrench']\n",
    "for name,model in models.items():\n",
    "    prediction_classes,true_classes,model_x= model\n",
    "    print(name)\n",
    "    print(classification_report(true_classes, prediction_classes, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
